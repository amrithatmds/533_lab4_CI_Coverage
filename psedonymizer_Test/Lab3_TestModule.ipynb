{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_anonymizer (__main__.TestAnonymizer) ... ok\n",
      "test_kval (__main__.TestAnonymizer) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create the class once for running all the tests\n",
      "Run the setUp for each testcase\n",
      "Completed running test_1 for the Anonymizer class\n",
      "Run the setUp for each testcase\n",
      "Completed running test_2 for the Anonymizer class\n",
      "Completed running all testcases for the Anonymizer class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.015s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1d78431fe48>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pseudonymizer import anonymizer\n",
    "import pandas as pd\n",
    "import unittest\n",
    "\n",
    "class TestAnonymizer(unittest.TestCase):\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    def setUp(self):\n",
    "        print (\"Run the setUp for each testcase\")\n",
    "        self.df = pd.read_csv(\"testDataset.csv\")   # sample testfile \n",
    "        self.classes = self.iden.suggest(self.df.columns)\n",
    "        self.quasis = self.classes['qId']\n",
    "        self.sensitives = self.classes[\"sensId\"]\n",
    "        TestAnonymizer.i = TestAnonymizer.i + 1\n",
    "        \n",
    "    def tearDown(self):\n",
    "        print (\"Completed running test_\" + str(TestAnonymizer.i) + \" for the Anonymizer class\")\n",
    "        \n",
    "    @classmethod    \n",
    "    def setUpClass(self):\n",
    "        print (\"Create the class once for running all the tests\")\n",
    "        self.iden = anonymizer.Anonymizer(\"k_anon\", \"counter\")\n",
    "        \n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        print (\"Completed running all testcases for the Anonymizer class\")\n",
    "        \n",
    "    def test_anonymizer(self):\n",
    "        \n",
    "        self.assertIn('DOB', self.quasis)\n",
    "        self.assertIn('postal_code', self.quasis)\n",
    "        self.assertIn('income', self.sensitives)\n",
    "        self.assertNotIn('companies', self.quasis)\n",
    "        self.assertNotIn('color', self.sensitives)\n",
    "    \n",
    "    def test_kval(self):\n",
    "        kval = self.iden.kcounter(self.df, self.quasis)\n",
    "        self.assertEqual(kval, 3)\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_evaluator (__main__.TestAnonymizer) ... ok\n",
      "test_evaluator (__main__.TestEvaluator) ... ok\n",
      "test_evaluator2 (__main__.TestEvaluator) ... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up the once for running all the tests\n",
      "Run the setUp for each testcase\n",
      "Completed running tests for the Evaluator class\n",
      "Completed running all testcases for the Evaluator class\n",
      "Set up the once for running all the tests\n",
      "Run the setUp for each testcase\n",
      "Completed running test_1 for the Evaluator class\n",
      "Run the setUp for each testcase\n",
      "Completed running test_2 for the Evaluator class\n",
      "Completed running all testcases for the Evaluator class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.051s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1d7843815c0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pseudonymizer import evaluator\n",
    "import pandas as pd\n",
    "import unittest\n",
    "\n",
    "class TestEvaluator(unittest.TestCase):\n",
    "    \n",
    "    i=0\n",
    "    def setUp(self):\n",
    "        print (\"Run the setUp for each testcase\")\n",
    "        TestEvaluator.i = TestEvaluator.i + 1\n",
    "        \n",
    "    def tearDown(self):\n",
    "        print (\"Completed running test_\" + str(TestEvaluator.i) + \" for the Evaluator class\")\n",
    "        \n",
    "    @classmethod    \n",
    "    def setUpClass(self):\n",
    "        print (\"Set up the once for running all the tests\")\n",
    "        self.ldiv = evaluator.Ldiversity('l_diversity', 'counter')\n",
    "        \n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        print (\"Completed running all testcases for the Evaluator class\")\n",
    "        \n",
    "    def test_evaluator(self):\n",
    "        \n",
    "        # Run the testcase for the first test file\n",
    "        df = pd.read_csv(\"creditcard.csv\")    # sample testfile \n",
    "        quasis = ['DOB', 'postal_code', 'Sex']\n",
    "        self.ldiv.setQuasiId(quasis)\n",
    "        sensId = ['credit_security_code']\n",
    "        self.ldiv.setSensitiveId(sensId)\n",
    "        maxProb = self.ldiv.ldivMaxProb(df, quasis, sensId)\n",
    "        \n",
    "        # Retrieve the values set to the evaluator object to make sure they are set correctly \n",
    "        setquasis = self.ldiv.getQuasiId()\n",
    "        setsensId =  self.ldiv.getSensitiveId()\n",
    "        \n",
    "        self.assertIn('DOB', setquasis)\n",
    "        self.assertIn('postal_code', setquasis)\n",
    "        self.assertIn('Sex', setquasis)\n",
    "        self.assertIn('credit_security_code', setsensId)\n",
    "        self.assertEqual(maxProb, 0.5)\n",
    "        \n",
    "    def test_evaluator2(self):\n",
    "        \n",
    "        # Run the testcase for the first test file\n",
    "        df = pd.read_csv(\"testDataset.csv\")    # sample testfile \n",
    "        quasis = ['DOB', 'postal_code']\n",
    "        self.ldiv.setQuasiId(quasis)\n",
    "        sensId = ['income']\n",
    "        self.ldiv.setSensitiveId(sensId)\n",
    "        maxProb = self.ldiv.ldivMaxProb(df, quasis, sensId)\n",
    "        \n",
    "        # Retrieve the values set to the evaluator object to make sure they are set correctly \n",
    "        setquasis = self.ldiv.getQuasiId()\n",
    "        setsensId =  self.ldiv.getSensitiveId()\n",
    "        \n",
    "        self.assertIn('DOB', setquasis)\n",
    "        self.assertIn('postal_code', setquasis)\n",
    "        self.assertIn('income', setsensId)\n",
    "        self.assertNotIn('companies',setquasis)\n",
    "        self.assertEqual(maxProb, 0.3333333333333333)\n",
    "    \n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_evaluator (__main__.TestAnonymizer) ... ok\n",
      "test_evaluator (__main__.TestEvaluator) ... ok\n",
      "test_evaluator2 (__main__.TestEvaluator) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.059s\n",
      "\n",
      "OK\n",
      "test_anonymizer (anonymizer_test.TestAnonymizer) ... ok\n",
      "test_kval (anonymizer_test.TestAnonymizer) ... ok\n",
      "test_evaluator (__main__.TestEvaluator) ... ok\n",
      "test_evaluator2 (__main__.TestEvaluator) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.052s\n",
      "\n",
      "OK\n",
      "...."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up the once for running all the tests\n",
      "Run the setUp for each testcase\n",
      "Completed running tests for the Evaluator class\n",
      "Completed running all testcases for the Evaluator class\n",
      "Set up the once for running all the tests\n",
      "Run the setUp for each testcase\n",
      "Completed running test_3 for the Evaluator class\n",
      "Run the setUp for each testcase\n",
      "Completed running test_4 for the Evaluator class\n",
      "Completed running all testcases for the Evaluator class\n",
      "Set up the class once for running all the tests\n",
      "Run the setUp for each testcase\n",
      "Completed running test_1 for the Anonymizer class\n",
      "Run the setUp for each testcase\n",
      "Completed running test_2 for the Anonymizer class\n",
      "Completed running all testcases for the Anonymizer class\n",
      "Set up the once for running all the tests\n",
      "Run the setUp for each testcase\n",
      "Completed running test_5 for the Evaluator class\n",
      "Run the setUp for each testcase\n",
      "Completed running test_6 for the Evaluator class\n",
      "Completed running all testcases for the Evaluator class\n",
      "Set up the class once for running all the tests\n",
      "Run the setUp for each testcase\n",
      "Completed running test_3 for the Anonymizer class\n",
      "Run the setUp for each testcase\n",
      "Completed running test_4 for the Anonymizer class\n",
      "Completed running all testcases for the Anonymizer class\n",
      "Set up the once for running all the tests\n",
      "Run the setUp for each testcase\n",
      "Completed running test_1 for the Evaluator class\n",
      "Run the setUp for each testcase\n",
      "Completed running test_2 for the Evaluator class\n",
      "Completed running all testcases for the Evaluator class\n",
      "<unittest.runner.TextTestResult run=4 errors=0 failures=0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.050s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "from anonymizer_test import TestAnonymizer\n",
    "from evaluator_test import TestEvaluator\n",
    "\n",
    "def my_suite():\n",
    "    suite = unittest.TestSuite()\n",
    "    result = unittest.TestResult()\n",
    "    suite.addTest(unittest.makeSuite(TestAnonymizer))\n",
    "    suite.addTest(unittest.makeSuite(TestEvaluator))\n",
    "    runner = unittest.TextTestRunner()\n",
    "    print (runner.run(suite))\n",
    "    \n",
    "my_suite()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
